{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b76165e",
   "metadata": {},
   "source": [
    "# Finetuning d'un LLM √† la fa√ßon du Conte de Monte Christo\n",
    "\n",
    "L'objectif de ce projet et de s'entrainer √† faire du finetunig de LLM. Pour cela on utilisera la m√©thode du QLoRA afin de ne pas avoir √† entrainer totalement un LLM 'de z√©ro'.\n",
    "\n",
    "## Nettoyage du texte pour former un dataset : \n",
    "\n",
    "- Supprimer les pages inutiles (Titre, sommaire, notes, etc...) ==> rawText_*.txt\n",
    "- Ne garder que les dialogues ==> rawText_*txt\n",
    "- Identifier puis √©crire dans un fichier les citations du Comte de Monte Cristo (avec phrase pr√©c√©dente) ==> dataset_CMC.JSONL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e77ff332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 1 nettoy√© !\n",
      "Fichier 2 nettoy√© !\n",
      "Fichier 3 nettoy√© !\n",
      "Dialogues extraits dans le fichier 1 !\n",
      "Dialogues extraits dans le fichier 2 !\n",
      "Dialogues extraits dans le fichier 3 !\n"
     ]
    }
   ],
   "source": [
    "# Filtrage des dialogues (to cleanText_*.txt)\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "for i in range(1, 4):\n",
    "\n",
    "    nom_fichier = f\"media/raw/rawText_{i}.txt\"\n",
    "    with open(nom_fichier, 'r', encoding='utf-8') as f:\n",
    "        lignes = f.readlines()\n",
    "\n",
    "    # Filtrer les lignes vides\n",
    "    lignes_propres = [ligne for ligne in lignes if ligne.strip()]\n",
    "    \n",
    "    with open(f\"media/raw/cleanBlankLignes_{i}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.writelines(lignes_propres)\n",
    "    \n",
    "    print(f\"Fichier {i} nettoy√© !\")\n",
    "\n",
    "#  Filtrer les dialogues (contenus dans ¬´ ¬ª)\n",
    "\n",
    "for i in range(1, 4):\n",
    "\n",
    "    nom_fichier = f\"media/raw/cleanBlankLignes_{i}.txt\"\n",
    "    with open(nom_fichier, 'r', encoding='utf-8') as f:\n",
    "        contenu = f.read()\n",
    "\n",
    "    # Extraire les dialogues entre ¬´ et ¬ª\n",
    "    dialogues = re.findall(r'¬´(.*?)¬ª', contenu, re.DOTALL)\n",
    "\n",
    "    # √âcrire les dialogues extraits dans le fichier\n",
    "    with open(f\"media/raw/cleanText_{i}.txt\", 'w', encoding='utf-8') as f:\n",
    "        for dialogue in dialogues:\n",
    "            f.write(dialogue.strip() + '\\n')\n",
    "\n",
    "    print(f\"Dialogues extraits dans le fichier {i} !\")\n",
    "\n",
    "# Fusion en un seul fichier\n",
    "\n",
    "with open('media/cleanDataset.txt', 'w', encoding='utf-8') as fichier_final:\n",
    "    for i in range(1, 4):\n",
    "        nom_fichier = f\"media/raw/cleanText_{i}.txt\"\n",
    "        with open(nom_fichier, 'r', encoding='utf-8') as f:\n",
    "            contenu = f.read()\n",
    "            fichier_final.write(contenu + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed94b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reprise d√©tect√©e √† la ligne 22900.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Curation: 22900it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Extraction termin√©e ou mise en pause. Checkpoint : checkpoint_dantes.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# 1. Configuration\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Tu es un expert en curation de donn√©es litt√©raires. \n",
    "Ta mission : extraire les r√©pliques d'EDMOND DANT√àS (Le Comte).\n",
    "- Ignore les serviteurs.\n",
    "- D√©veloppe l'output en 3-5 phrases (style XIXe si√®cle).\n",
    "- R√©ponds UNIQUEMENT en JSON (liste d'objets).\"\"\"\n",
    "\n",
    "def run_extraction_with_resume():\n",
    "    input_path = \"media/cleanDataset.txt\" \n",
    "    output_path = \"dantes_dataset_elaborated.jsonl\"\n",
    "    checkpoint_path = \"checkpoint_dantes.txt\" # Notre marque-page\n",
    "    chunk_size = 100 \n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"‚ùå Erreur : {input_path} introuvable.\")\n",
    "        return\n",
    "\n",
    "    # Lecture du fichier source\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    # --- LOGIQUE DE REPRISE ---\n",
    "    start_index = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            try:\n",
    "                start_index = int(f.read().strip())\n",
    "                print(f\"üîÑ Reprise d√©tect√©e √† la ligne {start_index}.\")\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Checkpoint corrompu, on repart de z√©ro.\")\n",
    "\n",
    "    total_lines = len(lines)\n",
    "    total_steps = (total_lines - start_index + chunk_size - 1) // chunk_size\n",
    "\n",
    "    # 2. Boucle d'extraction\n",
    "    # On commence la boucle √† 'start_index'\n",
    "    with tqdm(total=total_lines, initial=start_index, desc=\"Curation\") as pbar:\n",
    "        for i in range(start_index, total_lines, chunk_size):\n",
    "            chunk = lines[i : i + chunk_size]\n",
    "            prompt_text = \"\\n\".join(chunk)\n",
    "            \n",
    "            try:\n",
    "                response = client.models.generate_content(\n",
    "                    model=\"gemini-2.0-flash\",\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        system_instruction=SYSTEM_PROMPT,\n",
    "                        response_mime_type=\"application/json\",\n",
    "                        temperature=0.7\n",
    "                    ),\n",
    "                    contents=f\"Extraits √† traiter :\\n{prompt_text}\"\n",
    "                )\n",
    "                \n",
    "                # Nettoyage et sauvegarde des donn√©es\n",
    "                raw_text = response.text.strip()\n",
    "                if raw_text.startswith(\"```\"):\n",
    "                    raw_text = raw_text.split(\"json\")[-1].split(\"```\")[0].strip()\n",
    "                \n",
    "                data = json.loads(raw_text)\n",
    "                \n",
    "                if isinstance(data, list) and data:\n",
    "                    with open(output_path, \"a\", encoding=\"utf-8\") as out_file:\n",
    "                        for entry in data:\n",
    "                            out_file.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                # --- MISE √Ä JOUR DU CHECKPOINT ---\n",
    "                # On enregistre l'indice du prochain bloc √† traiter\n",
    "                with open(checkpoint_path, \"w\") as f_check:\n",
    "                    f_check.write(str(i + chunk_size))\n",
    "\n",
    "                pbar.update(len(chunk))\n",
    "                time.sleep(6)\n",
    "\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"‚ö†Ô∏è Erreur lot ligne {i} : {e}\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    print(f\"\\n‚ú® Extraction termin√©e ou mise en pause. Checkpoint : {checkpoint_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_extraction_with_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a35534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation du fichier d'entrainement :\n",
    "\n",
    "import json\n",
    "\n",
    "def finalize_dataset(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as out:\n",
    "        \n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # On unifie les cl√©s (speaker ou character -> instruction)\n",
    "            # On utilise le contexte pour fabriquer une consigne\n",
    "            context = data.get(\"context\", \"Le Comte s'exprime avec noblesse.\")\n",
    "            dialogue = data.get(\"dialogue\", \"\")\n",
    "            \n",
    "            # Nouveau format \"Instruct\"\n",
    "            new_entry = {\n",
    "                \"instruction\": f\"R√©ponds avec le ton du Comte de Monte-Cristo. Contexte : {context}\",\n",
    "                \"output\": dialogue\n",
    "            }\n",
    "            \n",
    "            out.write(json.dumps(new_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "finalize_dataset(\"dantes_dataset_elaborated.jsonl\", \"CMC_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d86cbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage termin√© : 2718 lignes valides conserv√©es sur 3080.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def ultra_clean_dataset(input_file, output_file):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    # Liste de mots √† supprimer (incises courantes)\n",
    "    incises = [\", dit le comte\", \", dit-il\", \", s'√©cria-t-il\", \", r√©pondit Dant√®s\", \", reprit Monte-Cristo\"]\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            output = data.get(\"output\")\n",
    "\n",
    "            # 1. On ignore les vides ou null\n",
    "            if not output or output.strip() == \"\" or output == \"null\":\n",
    "                continue\n",
    "            \n",
    "            # 2. Nettoyage des incises\n",
    "            for incise in incises:\n",
    "                output = output.replace(incise, \"\")\n",
    "            \n",
    "            # 3. Nettoyage des guillemets et espaces\n",
    "            output = output.replace('¬´', '').replace('¬ª', '').replace('\"', '').strip()\n",
    "            \n",
    "            # 4. Mise √† jour et sauvegarde\n",
    "            data[\"output\"] = output\n",
    "            cleaned_data.append(data)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as out:\n",
    "        for entry in cleaned_data:\n",
    "            out.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "    print(f\"Nettoyage termin√© : {len(cleaned_data)} lignes valides conserv√©es sur 3080.\")\n",
    "\n",
    "ultra_clean_dataset(\"CMC_dataset.jsonl\", \"CMC_dataset_FINAL.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
