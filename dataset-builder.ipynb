{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b76165e",
   "metadata": {},
   "source": [
    "# Finetuning d'un LLM à la façon du Conte de Monte Christo\n",
    "\n",
    "L'objectif de ce projet et de s'entrainer à faire du finetunig de LLM. Pour cela on utilisera la méthode du QLoRA afin de ne pas avoir à entrainer totalement un LLM 'de zéro'.\n",
    "\n",
    "## Nettoyage du texte : \n",
    "\n",
    "Avant toute chose il faut supprimer les pages inutiles (Titre, sommaire, notes, etc...).\n",
    "\n",
    "## Découpage du texte en segments :\n",
    "\n",
    "Afin de pouvoir traiter le texte le LLM aura besoin d'un texte découpé en 'chunks' de 4000 à 8000 tokens avec un 'overlap' de 200 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7c2f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de rawText_1.txt...\n",
      "Traitement de rawText_2.txt...\n",
      "Traitement de rawText_3.txt...\n",
      "Traitement de rawText_4.txt...\n",
      "\n",
      "Total de segments à traiter : 374\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def prepare_all_chunks(folder_path, chunk_size=8000, overlap=500):\n",
    "    all_chunks = []\n",
    "    # On récupère les fichiers et on les trie pour garder l'ordre chronologique (Tome 1 à 4)\n",
    "    files = sorted([f for f in os.listdir(folder_path) if f.endswith('.txt')])\n",
    "    \n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Traitement de {file_name}...\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            # On ajoute une petite métadonnée pour le suivi\n",
    "            all_chunks.append({\n",
    "                \"source\": file_name,\n",
    "                \"content\": chunk\n",
    "            })\n",
    "            start += chunk_size - overlap\n",
    "            \n",
    "    return all_chunks\n",
    "\n",
    "chunks_to_process = prepare_all_chunks(\"media\")\n",
    "print(f\"\\nTotal de segments à traiter : {len(chunks_to_process)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from wsgiref import types\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "# Necessite l'installation du SDK Google GenAI : python pip install google-genai\n",
    "\n",
    "# Chargement des variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"La clé API Gemini n'est pas définie dans le fichier .env\")\n",
    "\n",
    "\n",
    "# Configuration du client\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Tu es un expert en littérature française. Ta mission est d'extraire les répliques du personnage du Comte de Monte-Cristo.\n",
    "Pour chaque réplique trouvée dans le texte :\n",
    "1. Crée un champ \"instruction\" qui contient la phrase ou l'action qui précède la réplique (le contexte).\n",
    "2. Crée un champ \"output\" qui contient la réplique exacte du Comte (retire les incises comme \"dit-il\").\n",
    "3. Conserve rigoureusement le style d'époque (vouvoiement, vocabulaire).\n",
    "\n",
    "Renvoie uniquement une LISTE d'objets JSON.\"\"\"\n",
    "\n",
    "def test_extraction_gemini(chunks, num_samples=5):\n",
    "    print(f\"Lancement du test Gemini sur {num_samples} segments...\")\n",
    "    \n",
    "    with open(\"test_dataset_gemini.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(num_samples):\n",
    "            print(f\"Traitement du segment {i+1}...\")\n",
    "            \n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\", # Vous pouvez aussi utiliser \"gemini-1.5-flash\"\n",
    "                config=types.GenerateContentConfig(\n",
    "                    system_instruction=SYSTEM_PROMPT,\n",
    "                    response_mime_type=\"application/json\", # Force le format JSON\n",
    "                    temperature=0\n",
    "                ),\n",
    "                contents=chunks[i]['content']\n",
    "            )\n",
    "            \n",
    "            # Gemini renvoie souvent une chaîne JSON représentant une liste\n",
    "            # On la nettoie pour l'écrire en format JSONL\n",
    "            try:\n",
    "                data = response.parsed # Si le SDK supporte le parsing auto\n",
    "                if not data:\n",
    "                    import json\n",
    "                    data = json.loads(response.text)\n",
    "                \n",
    "                for entry in data:\n",
    "                    f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur de parsing sur le segment {i} : {e}\")\n",
    "\n",
    "    print(\"\\nTest terminé ! Vérifiez 'test_dataset_gemini.jsonl'.\")\n",
    "\n",
    "# Exécution du test\n",
    "test_extraction_gemini(chunks_to_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
